{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation: GPT-4o Baseline Performance\n",
    "\n",
    "**Author:** [Your Name]\n",
    "\n",
    "**Date:** [Current Date]\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is part of the VSF-Med (Vulnerability Scoring Framework for Medical Vision-Language Models) research project. It evaluates the baseline performance of OpenAI's GPT-4o Vision model on unmodified chest X-ray images from the MIMIC-CXR dataset.\n",
    "\n",
    "### Purpose\n",
    "- Establish baseline performance of GPT-4o on standard (non-adversarial) medical imaging tasks\n",
    "- Process a variety of chest X-ray images with standard medical prompts\n",
    "- Record model responses for later vulnerability analysis\n",
    "- Provide a comparison point for evaluating the impact of adversarial prompts and visual perturbations\n",
    "\n",
    "### Workflow\n",
    "1. Set up the environment and OpenAI API connection\n",
    "2. Fetch original chest X-ray images and questions from the dataset\n",
    "3. Process images through GPT-4o with standard prompts\n",
    "4. Store responses in a database for later analysis\n",
    "5. Analyze initial performance metrics\n",
    "\n",
    "### Model Information\n",
    "- **Model**: GPT-4o (OpenAI)\n",
    "- **Architecture**: Large-scale multimodal transformer model with vision capabilities\n",
    "- **Parameters**: Estimated >1 trillion parameters\n",
    "- **Training Data**: Diverse internet data including medical content\n",
    "- **Purpose**: General-purpose AI assistant with vision capabilities, not specifically trained for medical imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Install Required Libraries\n",
    "\n",
    "First, we'll install all necessary libraries for API access, image processing, and database operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install openai pillow sqlalchemy psycopg2-binary pandas numpy matplotlib python-dotenv tqdm pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import base64\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "import time\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Add the src directory to the path for importing custom modules\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check platform for environment-specific settings\n",
    "import platform\n",
    "operating_system = platform.system()\n",
    "print(f\"Operating System: {operating_system}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configuration Setup\n",
    "\n",
    "Load configuration from YAML file and set up environment-specific settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load configuration\n",
    "config_path = os.path.join(parent_dir, 'src', 'config', 'default_config.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Configure paths based on operating system\n",
    "if operating_system == 'Darwin':  # macOS\n",
    "    base_dir = os.path.expanduser('~/data/mimic-cxr-jpg/2.1.0/files')\n",
    "    output_dir = os.path.expanduser('~/data/vsf-med/output')\n",
    "elif operating_system == 'Linux':\n",
    "    base_dir = '/data/mimic-cxr-jpg/2.1.0/files'\n",
    "    output_dir = '/data/vsf-med/output'\n",
    "else:  # Windows or other\n",
    "    base_dir = config['paths']['data_dir'].replace('${HOME}', os.path.expanduser('~'))\n",
    "    output_dir = config['paths']['output_dir'].replace('${HOME}', os.path.expanduser('~'))\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set up database connection\n",
    "db_config = config['database']\n",
    "db_password = os.environ.get('DB_PASSWORD', '')\n",
    "CONNECTION_STRING = f\"postgresql://{db_config['user']}:{db_password}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "\n",
    "# Set up OpenAI API\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set. Please set it in your .env file.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Get model configuration\n",
    "model_config = config['models']['gpt4o']\n",
    "model_name = model_config['name']\n",
    "temperature = model_config['temperature']\n",
    "max_tokens = model_config['max_tokens']\n",
    "system_prompt = model_config['system_prompt']\n",
    "\n",
    "print(f\"Data directory: {base_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Using model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Functions\n",
    "\n",
    "Set up functions to interact with the database for fetching questions and storing model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def fetch_questions(condition='original', limit=100):\n",
    "    \"\"\"\n",
    "    Fetch questions from the database based on condition.\n",
    "    \n",
    "    Args:\n",
    "        condition (str): Type of questions to fetch (original, adversarial, etc.)\n",
    "        limit (int): Maximum number of questions to fetch\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the questions\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT id, question_id, condition, text, image \n",
    "    FROM mimicxp.mimic_all_qns \n",
    "    WHERE condition = '{condition}' \n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(query, conn)\n",
    "    \n",
    "    print(f\"Fetched {len(df)} {condition} questions from database\")\n",
    "    return df\n",
    "\n",
    "def store_model_response(uid, question_id, question, question_category, \n",
    "                         actual_answer, model_name, model_answer, image_link):\n",
    "    \"\"\"\n",
    "    Store model response in the database.\n",
    "    \n",
    "    Args:\n",
    "        uid (str): Unique identifier for the source image\n",
    "        question_id (str): Question ID\n",
    "        question (str): The question text\n",
    "        question_category (str): Category of question (original, visual_perturb, text_attack)\n",
    "        actual_answer (str): Ground truth answer (if available)\n",
    "        model_name (str): Name of the model\n",
    "        model_answer (str): Model's response\n",
    "        image_link (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        int: ID of the inserted record\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    INSERT INTO mimicxp.model_responses_r2 \n",
    "    (uid, question_id, question, question_category, actual_answer, model_name, model_answer, image_link, created_at) \n",
    "    VALUES (:uid, :question_id, :question, :question_category, :actual_answer, :model_name, :model_answer, :image_link, NOW()) \n",
    "    RETURNING id\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'uid': uid,\n",
    "        'question_id': str(question_id),\n",
    "        'question': question,\n",
    "        'question_category': question_category,\n",
    "        'actual_answer': actual_answer,\n",
    "        'model_name': model_name,\n",
    "        'model_answer': model_answer,\n",
    "        'image_link': image_link\n",
    "    }\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query), params)\n",
    "        conn.commit()\n",
    "        record_id = result.fetchone()[0]\n",
    "    \n",
    "    return record_id\n",
    "\n",
    "def check_existing_response(uid, question_id, model_name, question_category):\n",
    "    \"\"\"\n",
    "    Check if a response already exists in the database.\n",
    "    \n",
    "    Args:\n",
    "        uid (str): Unique identifier for the source image\n",
    "        question_id (str): Question ID\n",
    "        model_name (str): Name of the model\n",
    "        question_category (str): Category of question\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if response exists, False otherwise\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT COUNT(*) FROM mimicxp.model_responses_r2 \n",
    "    WHERE uid = :uid AND question_id = :question_id AND model_name = :model_name AND question_category = :question_category\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'uid': uid,\n",
    "        'question_id': str(question_id),\n",
    "        'model_name': model_name,\n",
    "        'question_category': question_category\n",
    "    }\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query), params)\n",
    "        count = result.fetchone()[0]\n",
    "    \n",
    "    return count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPT API Functions\n",
    "\n",
    "Define functions for interacting with the OpenAI GPT-4o Vision API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encode image to base64 for API submission.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64-encoded image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_response(image_path, prompt, retries=3, delay=2):\n",
    "    \"\"\"\n",
    "    Generate a response from GPT-4o Vision for the given image and prompt.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        prompt (str): Text prompt\n",
    "        retries (int): Number of retry attempts\n",
    "        delay (int): Delay between retries in seconds\n",
    "        \n",
    "    Returns:\n",
    "        str: Model's response\n",
    "    \"\"\"\n",
    "    # Encode image to base64\n",
    "    base64_image = encode_image(image_path)\n",
    "    if base64_image is None:\n",
    "        return \"Error: Could not encode image\"\n",
    "    \n",
    "    # Prepare messages for API call\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Call API with retry logic\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        except openai.RateLimitError:\n",
    "            print(f\"Rate limit exceeded. Waiting {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # Exponential backoff\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response (attempt {attempt+1}/{retries}): {e}\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return \"Error: Failed to generate response after multiple attempts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Helper Functions\n",
    "\n",
    "Set up functions for loading and displaying images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image from file for display.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Loaded image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_image_with_response(image_path, question, response):\n",
    "    \"\"\"\n",
    "    Display an image alongside the question and model response.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        question (str): The question text\n",
    "        response (str): Model's response\n",
    "    \"\"\"\n",
    "    img = load_image(image_path)\n",
    "    if img is None:\n",
    "        print(\"Could not load image for display\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Question: {question}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Loop\n",
    "\n",
    "Process multiple X-ray images with baseline questions to establish model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fetch original questions for baseline evaluation\n",
    "original_questions = fetch_questions(condition='original', limit=50)\n",
    "\n",
    "# Display a few sample questions\n",
    "original_questions[['question_id', 'text']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_questions(questions_df, model_name, question_category=\"original\", total_limit=20):\n",
    "    \"\"\"\n",
    "    Process a batch of questions and images through the model.\n",
    "    \n",
    "    Args:\n",
    "        questions_df (pd.DataFrame): DataFrame of questions\n",
    "        model_name (str): Name of the model\n",
    "        question_category (str): Category of questions\n",
    "        total_limit (int): Maximum number of questions to process\n",
    "        \n",
    "    Returns:\n",
    "        list: List of response records\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Limit the number of questions to process\n",
    "    questions_to_process = questions_df.head(total_limit)\n",
    "    \n",
    "    for idx, row in tqdm(questions_to_process.iterrows(), total=len(questions_to_process)):\n",
    "        uid = row['id']\n",
    "        question_id = row['question_id']\n",
    "        question = row['text']\n",
    "        image_path = row['image']\n",
    "        \n",
    "        # Form full image path\n",
    "        full_image_path = os.path.join(base_dir, image_path)\n",
    "        \n",
    "        # Check if this has already been evaluated\n",
    "        if check_existing_response(uid, question_id, model_name, question_category):\n",
    "            print(f\"Skipping already processed question {question_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Check if image exists\n",
    "        if not os.path.exists(full_image_path):\n",
    "            print(f\"Image not found: {full_image_path}\")\n",
    "            continue\n",
    "                \n",
    "        # Generate response with rate limiting\n",
    "        response = generate_response(full_image_path, question)\n",
    "        \n",
    "        # Add delay to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Store response in database\n",
    "        try:\n",
    "            record_id = store_model_response(\n",
    "                uid=uid,\n",
    "                question_id=question_id,\n",
    "                question=question,\n",
    "                question_category=question_category,\n",
    "                actual_answer=None,  # No ground truth available\n",
    "                model_name=model_name,\n",
    "                model_answer=response,\n",
    "                image_link=image_path\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'record_id': record_id,\n",
    "                'question_id': question_id,\n",
    "                'question': question,\n",
    "                'response': response,\n",
    "                'image_path': full_image_path\n",
    "            })\n",
    "            \n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error storing response: {e}\")\n",
    "    \n",
    "    print(f\"Processed {processed_count} questions\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Process original questions\n",
    "# Note: This cell will make API calls and may take a while to complete\n",
    "# It also incurs costs for OpenAI API usage\n",
    "baseline_results = process_questions(\n",
    "    questions_df=original_questions,\n",
    "    model_name=model_name,\n",
    "    question_category=\"original\",\n",
    "    total_limit=10  # Process only 10 images to limit API costs for demonstration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis\n",
    "\n",
    "Analyze the model responses to establish a baseline for model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display some example results\n",
    "if baseline_results:\n",
    "    for i, result in enumerate(baseline_results[:3]):\n",
    "        print(f\"Example {i+1}:\")\n",
    "        display_image_with_response(\n",
    "            image_path=result['image_path'],\n",
    "            question=result['question'],\n",
    "            response=result['response']\n",
    "        )\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "else:\n",
    "    print(\"No results available to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fetch all responses from the database for analysis\n",
    "def fetch_model_responses(model_name, question_category=\"original\", limit=100):\n",
    "    \"\"\"\n",
    "    Fetch model responses from the database.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model\n",
    "        question_category (str): Category of questions\n",
    "        limit (int): Maximum number of responses to fetch\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the responses\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT id, uid, question_id, question, model_answer, image_link \n",
    "    FROM mimicxp.model_responses_r2 \n",
    "    WHERE model_name = '{model_name}' AND question_category = '{question_category}' \n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(query, conn)\n",
    "    \n",
    "    print(f\"Fetched {len(df)} responses from database\")\n",
    "    return df\n",
    "\n",
    "# Fetch and analyze responses\n",
    "responses_df = fetch_model_responses(model_name=model_name, question_category=\"original\", limit=100)\n",
    "if not responses_df.empty:\n",
    "    # Analyze response length\n",
    "    responses_df['response_length'] = responses_df['model_answer'].apply(len)\n",
    "    \n",
    "    print(f\"Average response length: {responses_df['response_length'].mean():.2f} characters\")\n",
    "    print(f\"Median response length: {responses_df['response_length'].median()} characters\")\n",
    "    print(f\"Min response length: {responses_df['response_length'].min()} characters\")\n",
    "    print(f\"Max response length: {responses_df['response_length'].max()} characters\")\n",
    "    \n",
    "    # Plot response length distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(responses_df['response_length'], bins=20, alpha=0.7)\n",
    "    plt.title('Distribution of Response Lengths')\n",
    "    plt.xlabel('Response Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Analysis of Responses\n",
    "\n",
    "Analyze the content of responses to identify patterns and medical terminology usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic text analysis of responses\n",
    "if not responses_df.empty:\n",
    "    # Join all responses into one text for analysis\n",
    "    all_responses = \" \".join(responses_df['model_answer'].tolist())\n",
    "    \n",
    "    # Count common medical terms\n",
    "    medical_terms = [\n",
    "        \"opacity\", \"pneumonia\", \"effusion\", \"consolidation\", \"atelectasis\",\n",
    "        \"nodule\", \"mass\", \"cardiomegaly\", \"edema\", \"pleural\", \"lung\",\n",
    "        \"heart\", \"chest\", \"rib\", \"pulmonary\", \"abnormality\", \"finding\"\n",
    "    ]\n",
    "    \n",
    "    term_counts = {}\n",
    "    for term in medical_terms:\n",
    "        term_counts[term] = all_responses.lower().count(term)\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_terms = sorted(term_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Plot term frequencies\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    terms, counts = zip(*sorted_terms)\n",
    "    plt.bar(terms, counts)\n",
    "    plt.title('Medical Term Frequency in GPT-4o Responses')\n",
    "    plt.xlabel('Medical Terms')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've established the baseline performance of the GPT-4o Vision model on standard chest X-ray questions. This provides a foundation for comparing performance with adversarial inputs in subsequent notebooks.\n",
    "\n",
    "### Key Findings\n",
    "- Established GPT-4o baseline performance on MIMIC-CXR images\n",
    "- Documented response patterns for standard medical imaging questions\n",
    "- Analyzed frequency of medical terminology in model outputs\n",
    "- Stored baseline responses in the database for later comparison\n",
    "\n",
    "### Next Steps\n",
    "- Proceed to notebook `05_vulnerability_scoring_framework.ipynb` to apply the VSF-Med framework to evaluate model vulnerabilities\n",
    "- Analyze baseline performance in comparison with specialist models like CheXagent\n",
    "- Test GPT-4o on adversarial prompts and perturbed images\n",
    "- Compare with other models like Claude and Gemini\n",
    "\n",
    "This baseline serves as the control condition for our vulnerability assessment, allowing us to isolate the effects of adversarial inputs in later analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}