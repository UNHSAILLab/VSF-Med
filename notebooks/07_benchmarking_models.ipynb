{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Models: Comprehensive Performance Comparison\n",
    "\n",
    "**Author:** [Your Name]\n",
    "\n",
    "**Date:** [Current Date]\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is part of the VSF-Med (Vulnerability Scoring Framework for Medical Vision-Language Models) research project. It performs a comprehensive benchmarking analysis across all models evaluated in the study.\n",
    "\n",
    "### Purpose\n",
    "- Compare performance metrics across different models (GPT-4o, Claude, CheXagent)\n",
    "- Analyze vulnerability patterns across multiple attack vectors\n",
    "- Generate comprehensive cross-model performance comparisons\n",
    "- Identify strengths and weaknesses of each model\n",
    "- Create publication-quality visualizations for research presentation\n",
    "\n",
    "### Workflow\n",
    "1. Fetch evaluation data for all models from the database\n",
    "2. Calculate comparative metrics for model performance\n",
    "3. Analyze performance across different vulnerability dimensions\n",
    "4. Generate visualizations showing cross-model comparisons\n",
    "5. Identify patterns and insights about model vulnerabilities\n",
    "\n",
    "### Models Compared\n",
    "- **GPT-4o Vision** (OpenAI): General-purpose multimodal model\n",
    "- **Claude Opus** (Anthropic): General-purpose multimodal model\n",
    "- **CheXagent-8b** (StanfordAIMI): Specialized medical imaging model\n",
    "- **Additional models** (if available): Third-party or open models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Install Required Libraries\n",
    "\n",
    "First, we'll install all necessary libraries for data analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn sqlalchemy psycopg2-binary python-dotenv pyyaml scikit-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Add the src directory to the path for importing custom modules\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up plotting defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Check platform for environment-specific settings\n",
    "import platform\n",
    "operating_system = platform.system()\n",
    "print(f\"Operating System: {operating_system}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configuration Setup\n",
    "\n",
    "Load configuration from YAML file and set up environment-specific settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load configuration\n",
    "config_path = os.path.join(parent_dir, 'src', 'config', 'default_config.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Configure paths based on operating system\n",
    "if operating_system == 'Darwin':  # macOS\n",
    "    output_dir = os.path.expanduser('~/data/vsf-med/output')\n",
    "elif operating_system == 'Linux':\n",
    "    output_dir = '/data/vsf-med/output'\n",
    "else:  # Windows or other\n",
    "    output_dir = config['paths']['output_dir'].replace('${HOME}', os.path.expanduser('~'))\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "figures_dir = os.path.join(output_dir, 'figures')\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "\n",
    "# Set up database connection\n",
    "db_config = config['database']\n",
    "db_password = os.environ.get('DB_PASSWORD', '')\n",
    "CONNECTION_STRING = f\"postgresql://{db_config['user']}:{db_password}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "\n",
    "# Get evaluation dimensions and risk classifications\n",
    "dimensions = config['evaluation']['dimensions']\n",
    "risk_classifications = config['evaluation']['risk_classifications']\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Figures directory: {figures_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "Fetch and prepare data for all models from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def fetch_all_evaluations():\n",
    "    \"\"\"\n",
    "    Fetch all model evaluations from the database.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all evaluations\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT e.*, r.model_name, r.question_category, r.question, r.model_answer\n",
    "    FROM mimicxp.evaluations e\n",
    "    JOIN mimicxp.model_responses_r2 r ON e.model_response_id = r.id\n",
    "    ORDER BY e.created_at DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(query, conn)\n",
    "    \n",
    "    print(f\"Fetched {len(df)} evaluations from database\")\n",
    "    return df\n",
    "\n",
    "def fetch_model_responses():\n",
    "    \"\"\"\n",
    "    Fetch all model responses from the database.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all model responses\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT id, uid, question_id, question, question_category, model_name, model_answer, image_link, created_at\n",
    "    FROM mimicxp.model_responses_r2\n",
    "    ORDER BY created_at DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(query, conn)\n",
    "    \n",
    "    print(f\"Fetched {len(df)} model responses from database\")\n",
    "    return df\n",
    "\n",
    "# Fetch all evaluations and responses\n",
    "all_evaluations = fetch_all_evaluations()\n",
    "all_responses = fetch_model_responses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data for analysis\n",
    "def preprocess_data(evaluations_df, responses_df):\n",
    "    \"\"\"\n",
    "    Preprocess and prepare data for analysis.\n",
    "    \n",
    "    Args:\n",
    "        evaluations_df (pd.DataFrame): DataFrame with evaluations\n",
    "        responses_df (pd.DataFrame): DataFrame with responses\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with processed DataFrames\n",
    "    \"\"\"\n",
    "    # Check if we have data\n",
    "    if evaluations_df.empty or responses_df.empty:\n",
    "        print(\"Warning: Empty data. Cannot proceed with analysis.\")\n",
    "        return {}\n",
    "        \n",
    "    # Add category type (text attack, visual perturbation, original)\n",
    "    def categorize_question(category):\n",
    "        if category == 'original':\n",
    "            return 'baseline'\n",
    "        elif category.startswith('visual_perturb'):\n",
    "            return 'visual_perturbation'\n",
    "        elif category.startswith('text_attack'):\n",
    "            return 'text_attack'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    # Add category to evaluations\n",
    "    evaluations_df['category_type'] = evaluations_df['question_category'].apply(categorize_question)\n",
    "    \n",
    "    # Add category to responses\n",
    "    responses_df['category_type'] = responses_df['question_category'].apply(categorize_question)\n",
    "    \n",
    "    # Get response lengths\n",
    "    responses_df['response_length'] = responses_df['model_answer'].apply(len)\n",
    "    \n",
    "    # Create maps between score columns and dimension names\n",
    "    dimension_map = {\n",
    "        'prompt_injection_score': 'Prompt Injection Effectiveness',\n",
    "        'jailbreak_score': 'Jailbreak Success',\n",
    "        'confidentiality_score': 'Confidentiality Breach',\n",
    "        'misinformation_score': 'Misinformation/Malicious',\n",
    "        'dos_resilience_score': 'DoS Resilience',\n",
    "        'persistence_score': 'Persistence of Effects',\n",
    "        'safety_bypass_score': 'Safety Bypass Success',\n",
    "        'medical_impact_score': 'Medical Impact'\n",
    "    }\n",
    "    \n",
    "    # Create list of dimension score columns\n",
    "    dimension_cols = list(dimension_map.keys())\n",
    "    \n",
    "    # Group evaluations by model and category\n",
    "    model_category_groups = evaluations_df.groupby(['model_name', 'category_type'])\n",
    "    \n",
    "    # Calculate average scores by model and category\n",
    "    avg_scores = model_category_groups[dimension_cols + ['total_score']].mean().reset_index()\n",
    "    \n",
    "    # Calculate response stats by model and category\n",
    "    response_stats = responses_df.groupby(['model_name', 'category_type'])['response_length'].agg(\n",
    "        ['count', 'mean', 'std', 'min', 'max']\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Return processed data\n",
    "    return {\n",
    "        'evaluations': evaluations_df,\n",
    "        'responses': responses_df,\n",
    "        'avg_scores': avg_scores,\n",
    "        'response_stats': response_stats,\n",
    "        'dimension_map': dimension_map,\n",
    "        'dimension_cols': dimension_cols\n",
    "    }\n",
    "\n",
    "# Process data\n",
    "data = preprocess_data(all_evaluations, all_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Overview\n",
    "\n",
    "Generate a high-level overview of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create summary of model performance\n",
    "if data:\n",
    "    # Get unique models and count samples\n",
    "    evaluations = data['evaluations']\n",
    "    models = evaluations['model_name'].unique()\n",
    "    \n",
    "    print(f\"Models evaluated: {len(models)}\")\n",
    "    model_counts = evaluations['model_name'].value_counts()\n",
    "    \n",
    "    for model, count in model_counts.items():\n",
    "        print(f\"  {model}: {count} evaluations\")\n",
    "    \n",
    "    # Calculate overall average vulnerability score by model\n",
    "    model_scores = evaluations.groupby('model_name')['total_score'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "    model_scores = model_scores.sort_values(by='mean', ascending=True)\n",
    "    \n",
    "    print(\"\\nOverall vulnerability scores (lower is better):\")\n",
    "    for _, row in model_scores.iterrows():\n",
    "        print(f\"  {row['model_name']}: {row['mean']:.2f} ± {row['std']:.2f} (range: {row['min']}-{row['max']})\")\n",
    "    \n",
    "    # Create visual comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(model_scores['model_name'], model_scores['mean'], yerr=model_scores['std'], capsize=10, alpha=0.7)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Average Vulnerability Score (0-32)')\n",
    "    plt.title('Overall Vulnerability Scores by Model')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(figures_dir, 'overall_vulnerability_scores.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare performance across category types\n",
    "if data:\n",
    "    # Get data by category type\n",
    "    avg_scores = data['avg_scores']\n",
    "    dimension_cols = data['dimension_cols']\n",
    "    \n",
    "    # Create cross-tabulation of model vs category\n",
    "    model_category_pivot = avg_scores.pivot(index='model_name', columns='category_type', values='total_score')\n",
    "    \n",
    "    # Display the pivot table\n",
    "    print(\"Average vulnerability score by model and category type:\")\n",
    "    display(model_category_pivot)\n",
    "    \n",
    "    # Create bar chart comparing models across categories\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Convert pivot table to long format for seaborn\n",
    "    model_category_long = avg_scores[['model_name', 'category_type', 'total_score']]\n",
    "    \n",
    "    # Create bar chart\n",
    "    sns.barplot(data=model_category_long, x='model_name', y='total_score', hue='category_type')\n",
    "    plt.title('Vulnerability Score by Model and Category Type')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Average Vulnerability Score (0-32)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Category Type')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(figures_dir, 'vulnerability_by_category.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimension-Level Comparison\n",
    "\n",
    "Compare model performance across each vulnerability dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create radar chart for comparing models across dimensions\n",
    "if data:\n",
    "    # Prepare data for radar chart\n",
    "    dimension_cols = data['dimension_cols']\n",
    "    dimension_map = data['dimension_map']\n",
    "    evaluations = data['evaluations']\n",
    "    \n",
    "    # Calculate average scores by model\n",
    "    model_dimension_scores = evaluations.groupby('model_name')[dimension_cols].mean()\n",
    "    \n",
    "    # Create radar chart\n",
    "    def create_radar_chart(scores_df):\n",
    "        # Number of dimensions\n",
    "        N = len(dimension_cols)\n",
    "        \n",
    "        # Angle of each axis\n",
    "        angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Close the loop\n",
    "        \n",
    "        # Labels for each dimension\n",
    "        labels = [dimension_map[col] for col in dimension_cols]\n",
    "        labels += labels[:1]  # Close the loop\n",
    "        \n",
    "        # Create figure\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(111, polar=True)\n",
    "        \n",
    "        # Plot each model\n",
    "        for model_name, scores in scores_df.iterrows():\n",
    "            values = scores.values.tolist()\n",
    "            values += values[:1]  # Close the loop\n",
    "            \n",
    "            ax.plot(angles, values, linewidth=2, label=model_name)\n",
    "            ax.fill(angles, values, alpha=0.1)\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_xticks(angles)\n",
    "        ax.set_xticklabels(labels, fontsize=12)\n",
    "        \n",
    "        # Set y-axis limits\n",
    "        ax.set_ylim(0, 4)\n",
    "        plt.yticks([0, 1, 2, 3, 4], ['0', '1', '2', '3', '4'], fontsize=10)\n",
    "        \n",
    "        # Add legend\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "        \n",
    "        plt.title('Vulnerability Profile by Model Across Dimensions', size=15)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    # Create and save radar chart\n",
    "    radar_fig = create_radar_chart(model_dimension_scores)\n",
    "    radar_fig.savefig(os.path.join(figures_dir, 'model_radar_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create bar chart comparing models across dimensions\n",
    "    # Reshape data for plotting\n",
    "    dimension_comparison = model_dimension_scores.reset_index().melt(\n",
    "        id_vars=['model_name'],\n",
    "        value_vars=dimension_cols,\n",
    "        var_name='dimension',\n",
    "        value_name='score'\n",
    "    )\n",
    "    \n",
    "    # Map column names to readable dimension names\n",
    "    dimension_comparison['dimension'] = dimension_comparison['dimension'].map(dimension_map)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.barplot(data=dimension_comparison, x='dimension', y='score', hue='model_name')\n",
    "    plt.title('Vulnerability Score by Dimension and Model', fontsize=15)\n",
    "    plt.xlabel('Vulnerability Dimension', fontsize=12)\n",
    "    plt.ylabel('Average Score (0-4)', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Model')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(figures_dir, 'dimension_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify key vulnerabilities for each model\n",
    "if data:\n",
    "    # Get data\n",
    "    model_dimension_scores = data['evaluations'].groupby('model_name')[data['dimension_cols']].mean()\n",
    "    dimension_map = data['dimension_map']\n",
    "    \n",
    "    print(\"Key vulnerabilities by model (highest scoring dimensions):\")\n",
    "    for model, scores in model_dimension_scores.iterrows():\n",
    "        # Sort dimensions by score (descending)\n",
    "        sorted_dimensions = scores.sort_values(ascending=False)\n",
    "        \n",
    "        # Display top 3 vulnerabilities\n",
    "        print(f\"\\n{model}:\")\n",
    "        for dim, score in sorted_dimensions.head(3).items():\n",
    "            print(f\"  {dimension_map[dim]}: {score:.2f}/4\")\n",
    "            \n",
    "    # Create heatmap of vulnerabilities\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.heatmap(model_dimension_scores, annot=True, cmap='YlOrRd', vmin=0, vmax=4, fmt='.2f')\n",
    "    plt.title('Vulnerability Heatmap by Model and Dimension', fontsize=15)\n",
    "    plt.ylabel('Model', fontsize=12)\n",
    "    plt.xlabel('Vulnerability Dimension', fontsize=12)\n",
    "    \n",
    "    # Fix x-axis labels\n",
    "    ax.set_xticklabels([dimension_map[dim] for dim in data['dimension_cols']], rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(figures_dir, 'vulnerability_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attack Category Effectiveness\n",
    "\n",
    "Analyze which attack categories are most effective against each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze effectiveness of different text attack categories\n",
    "if data:\n",
    "    # Filter for text attack evaluations\n",
    "    text_attacks = data['evaluations'][data['evaluations']['category_type'] == 'text_attack']\n",
    "    \n",
    "    if not text_attacks.empty:\n",
    "        # Extract specific attack type from category\n",
    "        def extract_attack_type(category):\n",
    "            if category.startswith('text_attack_'):\n",
    "                return category.replace('text_attack_', '')\n",
    "            return category\n",
    "        \n",
    "        text_attacks['attack_type'] = text_attacks['question_category'].apply(extract_attack_type)\n",
    "        \n",
    "        # Calculate average score by model and attack type\n",
    "        attack_effectiveness = text_attacks.groupby(['model_name', 'attack_type'])['total_score'].mean().reset_index()\n",
    "        \n",
    "        # Create pivot table\n",
    "        attack_pivot = attack_effectiveness.pivot(index='attack_type', columns='model_name', values='total_score')\n",
    "        \n",
    "        # Sort by average effectiveness across models\n",
    "        attack_pivot['avg'] = attack_pivot.mean(axis=1)\n",
    "        attack_pivot = attack_pivot.sort_values('avg', ascending=False).drop('avg', axis=1)\n",
    "        \n",
    "        # Display top 10 most effective attacks\n",
    "        print(\"Top 10 most effective text attack categories:\")\n",
    "        display(attack_pivot.head(10))\n",
    "        \n",
    "        # Create heatmap of attack effectiveness\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(attack_pivot.head(10), annot=True, cmap='YlOrRd', fmt='.2f')\n",
    "        plt.title('Text Attack Effectiveness by Model (Top 10)', fontsize=15)\n",
    "        plt.xlabel('Model', fontsize=12)\n",
    "        plt.ylabel('Attack Type', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(figures_dir, 'text_attack_effectiveness.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze effectiveness of different visual perturbation types\n",
    "if data:\n",
    "    # Filter for visual perturbation evaluations\n",
    "    visual_perturbs = data['evaluations'][data['evaluations']['category_type'] == 'visual_perturbation']\n",
    "    \n",
    "    if not visual_perturbs.empty:\n",
    "        # Extract specific perturbation type from category\n",
    "        def extract_perturb_type(category):\n",
    "            if category.startswith('visual_perturb_'):\n",
    "                return category.replace('visual_perturb_', '')\n",
    "            return category\n",
    "        \n",
    "        visual_perturbs['perturb_type'] = visual_perturbs['question_category'].apply(extract_perturb_type)\n",
    "        \n",
    "        # Calculate average score by model and perturbation type\n",
    "        perturb_effectiveness = visual_perturbs.groupby(['model_name', 'perturb_type'])['total_score'].mean().reset_index()\n",
    "        \n",
    "        # Create pivot table\n",
    "        perturb_pivot = perturb_effectiveness.pivot(index='perturb_type', columns='model_name', values='total_score')\n",
    "        \n",
    "        # Sort by average effectiveness across models\n",
    "        perturb_pivot['avg'] = perturb_pivot.mean(axis=1)\n",
    "        perturb_pivot = perturb_pivot.sort_values('avg', ascending=False).drop('avg', axis=1)\n",
    "        \n",
    "        # Display most effective visual perturbations\n",
    "        print(\"Most effective visual perturbation types:\")\n",
    "        display(perturb_pivot)\n",
    "        \n",
    "        # Create bar chart of perturbation effectiveness\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Reshape for plotting\n",
    "        perturb_long = perturb_effectiveness.copy()\n",
    "        \n",
    "        # Create bar chart\n",
    "        sns.barplot(data=perturb_long, x='perturb_type', y='total_score', hue='model_name')\n",
    "        plt.title('Visual Perturbation Effectiveness by Model', fontsize=15)\n",
    "        plt.xlabel('Perturbation Type', fontsize=12)\n",
    "        plt.ylabel('Average Vulnerability Score', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Model')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(figures_dir, 'visual_perturbation_effectiveness.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Response Length Analysis\n",
    "\n",
    "Analyze response length patterns across models and attack types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze response length patterns\n",
    "if data:\n",
    "    # Get response data\n",
    "    responses = data['responses']\n",
    "    \n",
    "    # Calculate response length statistics by model and category\n",
    "    length_stats = responses.groupby(['model_name', 'category_type'])['response_length'].agg(\n",
    "        ['mean', 'std', 'count']\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Plot response length comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create bar chart with error bars\n",
    "    ax = sns.barplot(data=length_stats, x='model_name', y='mean', hue='category_type')\n",
    "    \n",
    "    # Add error bars\n",
    "    for i, bar in enumerate(ax.patches):\n",
    "        bar_idx = i // len(length_stats['category_type'].unique())\n",
    "        category_idx = i % len(length_stats['category_type'].unique())\n",
    "        \n",
    "        category_types = length_stats['category_type'].unique()\n",
    "        model_names = length_stats['model_name'].unique()\n",
    "        \n",
    "        if bar_idx < len(model_names) and category_idx < len(category_types):\n",
    "            std = length_stats[\n",
    "                (length_stats['model_name'] == model_names[bar_idx]) & \n",
    "                (length_stats['category_type'] == category_types[category_idx])\n",
    "            ]['std'].values\n",
    "            \n",
    "            if len(std) > 0:\n",
    "                x = bar.get_x() + bar.get_width() / 2\n",
    "                height = bar.get_height()\n",
    "                plt.errorbar(x, height, yerr=std[0], color='black', capsize=5)\n",
    "    \n",
    "    plt.title('Response Length by Model and Category Type', fontsize=15)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Average Response Length (characters)', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Category Type')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(figures_dir, 'response_length_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate percentage change in response length from baseline to adversarial\n",
    "    def calculate_length_changes(length_stats_df):\n",
    "        # Pivot the data\n",
    "        pivot_df = length_stats_df.pivot(index='model_name', columns='category_type', values='mean')\n",
    "        \n",
    "        # Calculate percentage changes\n",
    "        if 'baseline' in pivot_df.columns:\n",
    "            if 'text_attack' in pivot_df.columns:\n",
    "                pivot_df['text_attack_pct_change'] = (pivot_df['text_attack'] - pivot_df['baseline']) / pivot_df['baseline'] * 100\n",
    "            \n",
    "            if 'visual_perturbation' in pivot_df.columns:\n",
    "                pivot_df['visual_perturbation_pct_change'] = (pivot_df['visual_perturbation'] - pivot_df['baseline']) / pivot_df['baseline'] * 100\n",
    "        \n",
    "        return pivot_df\n",
    "    \n",
    "    # Calculate and display percentage changes\n",
    "    changes_df = calculate_length_changes(length_stats)\n",
    "    \n",
    "    print(\"\\nPercentage Change in Response Length from Baseline:\")\n",
    "    display(changes_df[['text_attack_pct_change', 'visual_perturbation_pct_change']])\n",
    "    \n",
    "    # Create bar chart of percentage changes\n",
    "    if 'text_attack_pct_change' in changes_df.columns or 'visual_perturbation_pct_change' in changes_df.columns:\n",
    "        # Reshape for plotting\n",
    "        change_cols = [col for col in ['text_attack_pct_change', 'visual_perturbation_pct_change'] if col in changes_df.columns]\n",
    "        changes_long = changes_df.reset_index().melt(\n",
    "            id_vars=['model_name'],\n",
    "            value_vars=change_cols,\n",
    "            var_name='attack_type',\n",
    "            value_name='percent_change'\n",
    "        )\n",
    "        \n",
    "        # Clean up attack type names\n",
    "        changes_long['attack_type'] = changes_long['attack_type'].str.replace('_pct_change', '')\n",
    "        \n",
    "        # Create bar chart\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(data=changes_long, x='model_name', y='percent_change', hue='attack_type')\n",
    "        plt.title('Percentage Change in Response Length from Baseline', fontsize=15)\n",
    "        plt.xlabel('Model', fontsize=12)\n",
    "        plt.ylabel('Percentage Change (%)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        plt.legend(title='Attack Type')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(figures_dir, 'response_length_pct_change.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Risk Classification Analysis\n",
    "\n",
    "Analyze the distribution of risk classifications for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze risk classification distributions\n",
    "if data:\n",
    "    # Get evaluation data\n",
    "    evaluations = data['evaluations']\n",
    "    \n",
    "    # Calculate risk distribution by model\n",
    "    risk_distribution = evaluations.groupby('model_name')['risk_classification'].value_counts(normalize=True).reset_index(name='percentage')\n",
    "    risk_distribution['percentage'] = risk_distribution['percentage'] * 100\n",
    "    \n",
    "    # Define risk order\n",
    "    risk_order = ['Low Risk', 'Moderate Risk', 'High Risk', 'Critical Risk']\n",
    "    \n",
    "    # Filter to keep only standard risk classifications\n",
    "    risk_distribution = risk_distribution[risk_distribution['risk_classification'].isin(risk_order)]\n",
    "    \n",
    "    # Create pivot table\n",
    "    risk_pivot = risk_distribution.pivot(index='model_name', columns='risk_classification', values='percentage')\n",
    "    \n",
    "    # Reorder columns by risk level\n",
    "    risk_pivot = risk_pivot.reindex(columns=risk_order)\n",
    "    \n",
    "    # Fill NaN with zeros\n",
    "    risk_pivot = risk_pivot.fillna(0)\n",
    "    \n",
    "    # Display risk distribution\n",
    "    print(\"Risk Classification Distribution by Model (percentage):\")\n",
    "    display(risk_pivot)\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    risk_pivot.plot(kind='bar', stacked=True, figsize=(12, 8), colormap='YlOrRd')\n",
    "    plt.title('Risk Classification Distribution by Model', fontsize=15)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Percentage (%)', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Risk Classification')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(figures_dir, 'risk_classification_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Vulnerability Profile Analysis\n",
    "\n",
    "Create comprehensive vulnerability profiles for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create comprehensive vulnerability profiles\n",
    "if data:\n",
    "    # Prepare data\n",
    "    evaluations = data['evaluations']\n",
    "    dimension_cols = data['dimension_cols']\n",
    "    dimension_map = data['dimension_map']\n",
    "    \n",
    "    # Create profile for each model\n",
    "    for model in evaluations['model_name'].unique():\n",
    "        model_data = evaluations[evaluations['model_name'] == model]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\\n\")\n",
    "        print(f\"Vulnerability Profile: {model}\\n\")\n",
    "        \n",
    "        # Overall stats\n",
    "        total_evaluations = len(model_data)\n",
    "        avg_score = model_data['total_score'].mean()\n",
    "        std_score = model_data['total_score'].std()\n",
    "        risk_counts = model_data['risk_classification'].value_counts(normalize=True) * 100\n",
    "        \n",
    "        print(f\"Total evaluations: {total_evaluations}\")\n",
    "        print(f\"Average vulnerability score: {avg_score:.2f} ± {std_score:.2f} (0-32 scale)\")\n",
    "        print(f\"Risk profile: {', '.join([f'{risk}: {pct:.1f}%' for risk, pct in risk_counts.items()])}\")\n",
    "        \n",
    "        # Dimension scores\n",
    "        dimension_scores = model_data[dimension_cols].mean()\n",
    "        \n",
    "        print(\"\\nVulnerability by dimension:\")\n",
    "        for dim, score in dimension_scores.sort_values(ascending=False).items():\n",
    "            print(f\"  {dimension_map[dim]}: {score:.2f}/4\")\n",
    "        \n",
    "        # Most vulnerable categories\n",
    "        category_scores = model_data.groupby('category_type')['total_score'].mean().sort_values(ascending=False)\n",
    "        \n",
    "        print(\"\\nVulnerability by category type:\")\n",
    "        for category, score in category_scores.items():\n",
    "            print(f\"  {category}: {score:.2f}/32\")\n",
    "            \n",
    "        # Most effective attacks (top 5)\n",
    "        if 'attack_type' not in model_data.columns and 'question_category' in model_data.columns:\n",
    "            model_data['attack_type'] = model_data['question_category'].apply(\n",
    "                lambda x: x.replace('text_attack_', '') if x.startswith('text_attack_') else \n",
    "                (x.replace('visual_perturb_', '') if x.startswith('visual_perturb_') else x)\n",
    "            )\n",
    "            \n",
    "        attack_scores = model_data.groupby('attack_type')['total_score'].mean().sort_values(ascending=False)\n",
    "        \n",
    "        print(\"\\nMost effective attacks (top 5):\")\n",
    "        for attack, score in attack_scores.head(5).items():\n",
    "            print(f\"  {attack}: {score:.2f}/32\")\n",
    "        \n",
    "        # Create radar chart for this model\n",
    "        N = len(dimension_cols)\n",
    "        angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Close the loop\n",
    "        \n",
    "        labels = [dimension_map[col] for col in dimension_cols]\n",
    "        labels += labels[:1]  # Close the loop\n",
    "        \n",
    "        values = dimension_scores.values.tolist()\n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(111, polar=True)\n",
    "        \n",
    "        ax.plot(angles, values, linewidth=2, label=model)\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "        \n",
    "        ax.set_xticks(angles)\n",
    "        ax.set_xticklabels(labels, fontsize=12)\n",
    "        \n",
    "        ax.set_ylim(0, 4)\n",
    "        plt.yticks([0, 1, 2, 3, 4], ['0', '1', '2', '3', '4'], fontsize=10)\n",
    "        \n",
    "        plt.title(f'Vulnerability Profile: {model}', size=15)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save model-specific radar chart\n",
    "        plt.savefig(os.path.join(figures_dir, f'{model.replace(\"/\", \"_\")}_vulnerability_profile.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results\n",
    "\n",
    "Export comprehensive benchmarking results for further analysis and publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Export comprehensive results to CSV\n",
    "if data:\n",
    "    # Create summary dataframe\n",
    "    models = data['evaluations']['model_name'].unique()\n",
    "    summary_rows = []\n",
    "    \n",
    "    for model in models:\n",
    "        model_data = data['evaluations'][data['evaluations']['model_name'] == model]\n",
    "        \n",
    "        # Overall stats\n",
    "        total_evaluations = len(model_data)\n",
    "        avg_score = model_data['total_score'].mean()\n",
    "        std_score = model_data['total_score'].std()\n",
    "        min_score = model_data['total_score'].min()\n",
    "        max_score = model_data['total_score'].max()\n",
    "        \n",
    "        # Risk classification\n",
    "        risk_counts = model_data['risk_classification'].value_counts(normalize=True) * 100\n",
    "        low_risk = risk_counts.get('Low Risk', 0)\n",
    "        moderate_risk = risk_counts.get('Moderate Risk', 0)\n",
    "        high_risk = risk_counts.get('High Risk', 0)\n",
    "        critical_risk = risk_counts.get('Critical Risk', 0)\n",
    "        \n",
    "        # Dimension scores\n",
    "        dimension_scores = {}\n",
    "        for dim in data['dimension_cols']:\n",
    "            dimension_scores[f\"{dim}_avg\"] = model_data[dim].mean()\n",
    "            dimension_scores[f\"{dim}_std\"] = model_data[dim].std()\n",
    "        \n",
    "        # Category scores\n",
    "        category_scores = {}\n",
    "        for category in model_data['category_type'].unique():\n",
    "            category_data = model_data[model_data['category_type'] == category]\n",
    "            category_scores[f\"{category}_avg\"] = category_data['total_score'].mean()\n",
    "            category_scores[f\"{category}_std\"] = category_data['total_score'].std()\n",
    "            category_scores[f\"{category}_count\"] = len(category_data)\n",
    "        \n",
    "        # Combine all data\n",
    "        summary_rows.append({\n",
    "            'model': model,\n",
    "            'total_evaluations': total_evaluations,\n",
    "            'avg_score': avg_score,\n",
    "            'std_score': std_score,\n",
    "            'min_score': min_score,\n",
    "            'max_score': max_score,\n",
    "            'low_risk_pct': low_risk,\n",
    "            'moderate_risk_pct': moderate_risk,\n",
    "            'high_risk_pct': high_risk,\n",
    "            'critical_risk_pct': critical_risk,\n",
    "            **dimension_scores,\n",
    "            **category_scores\n",
    "        })\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    \n",
    "    # Export to CSV\n",
    "    summary_path = os.path.join(output_dir, 'benchmarking_summary.csv')\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"Saved benchmarking summary to {summary_path}\")\n",
    "    \n",
    "    # Export raw data for further analysis\n",
    "    raw_data_path = os.path.join(output_dir, 'vsf_med_complete_dataset.csv')\n",
    "    data['evaluations'].to_csv(raw_data_path, index=False)\n",
    "    print(f\"Saved complete VSF-Med dataset to {raw_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "In this notebook, we've performed a comprehensive benchmarking analysis across all models evaluated in the VSF-Med framework. Our analysis provides insights into the relative vulnerability of different models to various types of adversarial inputs.\n",
    "\n",
    "### Key Findings\n",
    "- [Summarize which model performed best overall in terms of security]\n",
    "- [Identify the most common vulnerabilities across all models]\n",
    "- [Note which attack types were most effective]\n",
    "- [Highlight differences between general models (GPT, Claude) vs. specialized models (CheXagent)]\n",
    "- [Describe patterns in how models respond to different types of attacks]\n",
    "\n",
    "### Implications for Medical AI\n",
    "- These results highlight important security considerations for deploying vision-language models in clinical settings\n",
    "- The identified vulnerabilities suggest specific mitigations that should be implemented\n",
    "- Different models show different vulnerability profiles, suggesting different use cases may require different models\n",
    "\n",
    "### Next Steps\n",
    "- Proceed to notebook `08_analysis_radiologist_comparison.ipynb` to compare model performance with radiologists\n",
    "- Consider implementing suggested mitigations and re-evaluating model performance\n",
    "- Explore additional attack vectors and defense mechanisms\n",
    "\n",
    "This comprehensive benchmarking provides a foundation for understanding the security posture of current medical vision-language models and informs future development of more robust systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}