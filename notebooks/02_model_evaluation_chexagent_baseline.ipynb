{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation: CheXagent Baseline Performance\n",
    "\n",
    "**Author:** [Your Name]\n",
    "\n",
    "**Date:** [Current Date]\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is part of the VSF-Med (Vulnerability Scoring Framework for Medical Vision-Language Models) research project. It evaluates the baseline performance of the StanfordAIMI/CheXagent-8b model on unmodified chest X-ray images from the MIMIC-CXR dataset.\n",
    "\n",
    "### Purpose\n",
    "- Establish baseline performance of the CheXagent model on standard (non-adversarial) inputs\n",
    "- Process a variety of chest X-ray images with medical prompts\n",
    "- Record model responses for later vulnerability analysis\n",
    "- Provide a comparison point for evaluating the impact of adversarial prompts and visual perturbations\n",
    "\n",
    "### Workflow\n",
    "1. Set up the environment and load the CheXagent model\n",
    "2. Fetch original chest X-ray images and questions from the dataset\n",
    "3. Process images through the model with standard prompts\n",
    "4. Store responses in a database for later analysis\n",
    "5. Analyze initial performance metrics\n",
    "\n",
    "### Model Information\n",
    "- **Model**: StanfordAIMI/CheXagent-8b\n",
    "- **Architecture**: Vision-Language Model optimized for chest radiograph interpretation\n",
    "- **Parameters**: 8 billion parameters\n",
    "- **Training Data**: Medical imaging datasets including chest X-rays\n",
    "- **Purpose**: Assist in medical image interpretation and diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Install Required Libraries\n",
    "\n",
    "First, we'll install all necessary libraries for model inference, image processing, and database operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers pillow sqlalchemy psycopg2-binary pandas numpy matplotlib python-dotenv tqdm pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "# Add the src directory to the path for importing custom modules\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check platform for environment-specific settings\n",
    "import platform\n",
    "operating_system = platform.system()\n",
    "print(f\"Operating System: {operating_system}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configuration Setup\n",
    "\n",
    "Load configuration from YAML file and set up environment-specific settings (paths, database connection, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load configuration\n",
    "config_path = os.path.join(parent_dir, 'src', 'config', 'default_config.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Configure paths based on operating system\n",
    "if operating_system == 'Darwin':  # macOS\n",
    "    base_dir = os.path.expanduser('~/data/mimic-cxr-jpg/2.1.0/files')\n",
    "    output_dir = os.path.expanduser('~/data/vsf-med/output')\n",
    "elif operating_system == 'Linux':\n",
    "    base_dir = '/data/mimic-cxr-jpg/2.1.0/files'\n",
    "    output_dir = '/data/vsf-med/output'\n",
    "else:  # Windows or other\n",
    "    base_dir = config['paths']['data_dir'].replace('${HOME}', os.path.expanduser('~'))\n",
    "    output_dir = config['paths']['output_dir'].replace('${HOME}', os.path.expanduser('~'))\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set up database connection\n",
    "db_config = config['database']\n",
    "db_password = os.environ.get('DB_PASSWORD', '')\n",
    "CONNECTION_STRING = f\"postgresql://{db_config['user']}:{db_password}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "\n",
    "print(f\"Data directory: {base_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Functions\n",
    "\n",
    "Set up functions to interact with the database for fetching questions and storing model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def fetch_questions(condition='original', limit=100):\n",
    "    \"\"\"\n",
    "    Fetch questions from the database based on condition.\n",
    "    \n",
    "    Args:\n",
    "        condition (str): Type of questions to fetch (original, adversarial, etc.)\n",
    "        limit (int): Maximum number of questions to fetch\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the questions\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT id, question_id, condition, text, image \n",
    "    FROM mimicxp.mimic_all_qns \n",
    "    WHERE condition = '{condition}' \n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(query, conn)\n",
    "    \n",
    "    print(f\"Fetched {len(df)} {condition} questions from database\")\n",
    "    return df\n",
    "\n",
    "def store_model_response(uid, question_id, question, question_category, \n",
    "                         actual_answer, model_name, model_answer, image_link):\n",
    "    \"\"\"\n",
    "    Store model response in the database.\n",
    "    \n",
    "    Args:\n",
    "        uid (str): Unique identifier for the source image\n",
    "        question_id (str): Question ID\n",
    "        question (str): The question text\n",
    "        question_category (str): Category of question (original, visual_perturb, text_attack)\n",
    "        actual_answer (str): Ground truth answer (if available)\n",
    "        model_name (str): Name of the model\n",
    "        model_answer (str): Model's response\n",
    "        image_link (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        int: ID of the inserted record\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    INSERT INTO mimicxp.model_responses_r2 \n",
    "    (uid, question_id, question, question_category, actual_answer, model_name, model_answer, image_link, created_at) \n",
    "    VALUES (:uid, :question_id, :question, :question_category, :actual_answer, :model_name, :model_answer, :image_link, NOW()) \n",
    "    RETURNING id\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'uid': uid,\n",
    "        'question_id': str(question_id),\n",
    "        'question': question,\n",
    "        'question_category': question_category,\n",
    "        'actual_answer': actual_answer,\n",
    "        'model_name': model_name,\n",
    "        'model_answer': model_answer,\n",
    "        'image_link': image_link\n",
    "    }\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query), params)\n",
    "        conn.commit()\n",
    "        record_id = result.fetchone()[0]\n",
    "    \n",
    "    return record_id\n",
    "\n",
    "def check_existing_response(uid, question_id, model_name, question_category):\n",
    "    \"\"\"\n",
    "    Check if a response already exists in the database.\n",
    "    \n",
    "    Args:\n",
    "        uid (str): Unique identifier for the source image\n",
    "        question_id (str): Question ID\n",
    "        model_name (str): Name of the model\n",
    "        question_category (str): Category of question\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if response exists, False otherwise\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT COUNT(*) FROM mimicxp.model_responses_r2 \n",
    "    WHERE uid = :uid AND question_id = :question_id AND model_name = :model_name AND question_category = :question_category\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'uid': uid,\n",
    "        'question_id': str(question_id),\n",
    "        'model_name': model_name,\n",
    "        'question_category': question_category\n",
    "    }\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query), params)\n",
    "        count = result.fetchone()[0]\n",
    "    \n",
    "    return count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Setup\n",
    "\n",
    "Load the CheXagent model and set up for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def setup_model():\n",
    "    \"\"\"\n",
    "    Set up the CheXagent model for inference.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, processor)\n",
    "    \"\"\"\n",
    "    model_id = \"StanfordAIMI/CheXagent-8b\"\n",
    "    print(f\"Loading model: {model_id}\")\n",
    "    \n",
    "    # Check for GPU availability\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model and processor\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None\n",
    "    )\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "# Load the model\n",
    "try:\n",
    "    model, processor = setup_model()\n",
    "    print(\"Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    # For demonstration, continue execution without model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference Functions\n",
    "\n",
    "Define functions for processing images and generating responses from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image from file.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: Loaded image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_response(model, processor, image, prompt):\n",
    "    \"\"\"\n",
    "    Generate a response from the model for the given image and prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: CheXagent model\n",
    "        processor: CheXagent processor\n",
    "        image (PIL.Image): Input image\n",
    "        prompt (str): Text prompt\n",
    "        \n",
    "    Returns:\n",
    "        str: Model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Process the image and prompt\n",
    "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.1,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "        \n",
    "        # Decode and clean the response\n",
    "        response = processor.decode(output[0], skip_special_tokens=True)\n",
    "        response = response.replace(prompt, \"\").strip()\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"\n",
    "    Clear GPU memory to prevent out-of-memory errors during batch processing.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Cleared GPU memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Loop\n",
    "\n",
    "Process multiple X-ray images with baseline questions to establish model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fetch original questions for baseline evaluation\n",
    "original_questions = fetch_questions(condition='original', limit=200)\n",
    "\n",
    "# Display a few sample questions\n",
    "original_questions[['question_id', 'text']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_batch(questions_df, model_name=\"StanfordAIMI/CheXagent-8b\", question_category=\"original\", batch_size=10, total_limit=50):\n",
    "    \"\"\"\n",
    "    Process a batch of questions and images through the model.\n",
    "    \n",
    "    Args:\n",
    "        questions_df (pd.DataFrame): DataFrame of questions\n",
    "        model_name (str): Name of the model\n",
    "        question_category (str): Category of questions\n",
    "        batch_size (int): Number of questions to process before clearing memory\n",
    "        total_limit (int): Maximum number of questions to process\n",
    "        \n",
    "    Returns:\n",
    "        list: List of response records\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Limit the number of questions to process\n",
    "    questions_to_process = questions_df.head(total_limit)\n",
    "    \n",
    "    for batch_start in range(0, len(questions_to_process), batch_size):\n",
    "        # Get batch of questions\n",
    "        batch_end = min(batch_start + batch_size, len(questions_to_process))\n",
    "        batch = questions_to_process.iloc[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"Processing batch {batch_start//batch_size + 1}/{(len(questions_to_process)-1)//batch_size + 1}\")\n",
    "        \n",
    "        for _, row in tqdm(batch.iterrows(), total=len(batch)):\n",
    "            uid = row['id']\n",
    "            question_id = row['question_id']\n",
    "            question = row['text']\n",
    "            image_path = row['image']\n",
    "            \n",
    "            # Form full image path\n",
    "            full_image_path = os.path.join(base_dir, image_path)\n",
    "            \n",
    "            # Check if this has already been evaluated\n",
    "            if check_existing_response(uid, question_id, model_name, question_category):\n",
    "                print(f\"Skipping already processed question {question_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Load image\n",
    "            img = load_image(full_image_path)\n",
    "            if img is None:\n",
    "                print(f\"Skipping image {image_path} due to loading error\")\n",
    "                continue\n",
    "                \n",
    "            # Generate response\n",
    "            response = generate_response(model, processor, img, question)\n",
    "            \n",
    "            # Store response in database\n",
    "            try:\n",
    "                record_id = store_model_response(\n",
    "                    uid=uid,\n",
    "                    question_id=question_id,\n",
    "                    question=question,\n",
    "                    question_category=question_category,\n",
    "                    actual_answer=None,  # No ground truth available\n",
    "                    model_name=model_name,\n",
    "                    model_answer=response,\n",
    "                    image_link=image_path\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    'record_id': record_id,\n",
    "                    'question_id': question_id,\n",
    "                    'question': question,\n",
    "                    'response': response\n",
    "                })\n",
    "                \n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error storing response: {e}\")\n",
    "        \n",
    "        # Clear GPU memory after each batch\n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    print(f\"Processed {processed_count} questions\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Process a batch of original questions\n",
    "# Note: This cell will process images and may take a while to complete\n",
    "baseline_results = process_batch(\n",
    "    questions_df=original_questions,\n",
    "    model_name=\"StanfordAIMI/CheXagent-8b\",\n",
    "    question_category=\"original\",\n",
    "    batch_size=5,  # Process 5 images before clearing memory\n",
    "    total_limit=20  # Process up to 20 images in total\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis\n",
    "\n",
    "Analyze the model responses to establish a baseline for model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display some example results\n",
    "if baseline_results:\n",
    "    for i, result in enumerate(baseline_results[:3]):\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"Question: {result['question']}\")\n",
    "        print(f\"Response: {result['response']}\")\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "else:\n",
    "    print(\"No results available to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fetch all responses from the database for analysis\n",
    "def fetch_model_responses(model_name=\"StanfordAIMI/CheXagent-8b\", question_category=\"original\", limit=100):\n",
    "    \"\"\"\n",
    "    Fetch model responses from the database.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model\n",
    "        question_category (str): Category of questions\n",
    "        limit (int): Maximum number of responses to fetch\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the responses\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT id, uid, question_id, question, model_answer, image_link \n",
    "    FROM mimicxp.model_responses_r2 \n",
    "    WHERE model_name = '{model_name}' AND question_category = '{question_category}' \n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        df = pd.read_sql(query, conn)\n",
    "    \n",
    "    print(f\"Fetched {len(df)} responses from database\")\n",
    "    return df\n",
    "\n",
    "# Fetch and analyze responses\n",
    "responses_df = fetch_model_responses(model_name=\"StanfordAIMI/CheXagent-8b\", question_category=\"original\", limit=100)\n",
    "if not responses_df.empty:\n",
    "    # Analyze response length\n",
    "    responses_df['response_length'] = responses_df['model_answer'].apply(len)\n",
    "    \n",
    "    print(f\"Average response length: {responses_df['response_length'].mean():.2f} characters\")\n",
    "    print(f\"Median response length: {responses_df['response_length'].median()} characters\")\n",
    "    print(f\"Min response length: {responses_df['response_length'].min()} characters\")\n",
    "    print(f\"Max response length: {responses_df['response_length'].max()} characters\")\n",
    "    \n",
    "    # Plot response length distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(responses_df['response_length'], bins=20, alpha=0.7)\n",
    "    plt.title('Distribution of Response Lengths')\n",
    "    plt.xlabel('Response Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've established the baseline performance of the CheXagent-8b model on standard chest X-ray questions. This provides a foundation for comparing performance with adversarial inputs in subsequent notebooks.\n",
    "\n",
    "### Key Findings\n",
    "- Established CheXagent-8b baseline performance on MIMIC-CXR images\n",
    "- Documented response patterns for standard medical imaging questions\n",
    "- Stored baseline responses in the database for later comparison\n",
    "\n",
    "### Next Steps\n",
    "- Proceed to notebook `03_model_evaluation_chexagent_perturbed.ipynb` to evaluate the model on visually perturbed images\n",
    "- Then compare with other models in subsequent notebooks\n",
    "- Finally, use the VSF-Med framework to comprehensively evaluate vulnerabilities\n",
    "\n",
    "This baseline serves as the control condition for our vulnerability assessment, allowing us to isolate the effects of adversarial inputs in later analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}